{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reminders-Classification-Module.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm-cNBkAfbso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "385fdc78-cd9c-4aa7-f926-60ab30bc3a52"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !git clone https://github.com/vikranth22446/assistant_intent_classifier\n",
        "# !cd /content/drive\n",
        "# !apt-get install git-lfs\n",
        "# !mkdir GloVe\n",
        "# !curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# !unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
        "\n",
        "# !mkdir encoder\n",
        "# !curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
        "\n",
        "%cd /content/drive/My Drive/assistant_intent_classifier\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/assistant_intent_classifier\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmhCCceZfgN_"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import subprocess\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import fileinput\n",
        "from contextlib import closing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Add, Dropout, Concatenate, Embedding, Bidirectional, LSTM, Flatten, Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "from models import InferSent\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from tensorflow.keras import regularizers\n",
        "import os\n",
        "import datetime\n",
        "np.random.seed(3252) \n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDrAtvjOhhO8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "0b033066-9304-4ace-8e5d-0fa393ae4c62"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 7.2MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 11.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81695 sha256=f6a6cd54f1011e89939aa7317d8b5197e807b88dccfcce59ec9b099d02ea2f96\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qu3wjsmfpCN"
      },
      "source": [
        "import pandas\n",
        "df_train = pandas.read_csv('df_train_squad_reminders.csv')\n",
        "df_test = pandas.read_csv('df_test_squad_reminders.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iPGRgnffs5t"
      },
      "source": [
        "import re\n",
        "import contractions \n",
        "\n",
        "def get_reminders(df):\n",
        "    reminders = []\n",
        "    for index, row in df.iterrows():\n",
        "        context = row['context'].replace('\"', '').replace(\".\", \". \").replace(\"?\", \"? \").lower()\n",
        "        processed = [contractions.fix(item).replace('.', \"\") for item in nltk.sent_tokenize(context)]\n",
        "        reminders = reminders + nltk.sent_tokenize(context)\n",
        "    return reminders\n",
        "\n",
        "X_train = get_reminders(df_train)\n",
        "y_train_one_hot = [[1, 0] for item in range(len(X_train))]\n",
        "\n",
        "X_val = get_reminders(df_test)\n",
        "y_val_one_hot = [[1, 0] for item in range(len(X_val))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN5U4HZQsUhf"
      },
      "source": [
        "X_train_reminder = X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NbmQYs2hoeV"
      },
      "source": [
        "import json\n",
        "import random\n",
        "f = open('data_processed_42_intents.json',)\n",
        "data = json.load(f)\n",
        "\n",
        "train_oos = []\n",
        "val_oos = []\n",
        "for i in data:\n",
        "    if i['intents'] == 'oos':\n",
        "        if random.random() < 2 * len(X_train)/len(data):\n",
        "            train_oos.append(i['text'])\n",
        "        if random.random() < 2 * len(X_val)/len(data):\n",
        "            val_oos.append(i['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyJcQeEssANY"
      },
      "source": [
        "X_train += train_oos\n",
        "y_train_one_hot += [[0, 1] for item in range(len(train_oos))] \n",
        "\n",
        "X_val += val_oos\n",
        "y_val_one_hot += [[0, 1] for item in range(len(val_oos))] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIFkuAEyyGNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e58c45b1-c5a8-464f-c59a-cd069e0ac20c"
      },
      "source": [
        "len(X_train), len(y_train_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4392, 4392)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PKs9UJDnxCg"
      },
      "source": [
        "# Load model\n",
        "%autoreload\n",
        "def generate_Infersent_model():\n",
        "    # Load model\n",
        "    model_version = 1\n",
        "    MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
        "    params_model = {'bsize': 64, \n",
        "                    'word_emb_dim': 300, \n",
        "                    'enc_lstm_dim': 2048,\n",
        "                    'pool_type': 'max',\n",
        "                    'dpout_model': 0.0, \n",
        "                    'version': model_version}\n",
        "    model = InferSent(params_model)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "    W2V_PATH = 'GloVe/glove.840B.300d.txt'\n",
        "    model.set_w2v_path(W2V_PATH)\n",
        "\n",
        "    model.build_vocab_k_words(K=100000)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_doc2vec(text, model, verbose=False):\n",
        "    emb = model.encode(text, verbose=verbose)\n",
        "    return emb\n",
        "\n",
        "def generate_basic_fully_connected():\n",
        "    input_1 = Input((4096,), dtype=tf.float32)\n",
        "    x = Dense(2000, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(input_1)\n",
        "    \n",
        "    x = Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    \n",
        "    x = Dense(450, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    \n",
        "    x = Dense(250, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    out = Dense(2, activation='softmax')(x)\n",
        "    dual_model = Model(inputs=input_1, outputs=out)\n",
        "    \n",
        "    adamOpti = Adam()\n",
        "    dual_model.compile(optimizer=adamOpti, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    dual_model.summary()\n",
        "    return dual_model\n",
        "\n",
        "def get_cnn_model(num_output=140):\n",
        "  # https://github.com/ajinkyaT/CNN_Intent_Classification/blob/master/Intent_Classification_Keras_Glove.ipynb\n",
        "    MAX_SEQUENCE_LENGTH = 10 # Maximum number of words in a sentence\n",
        "    MAX_NB_WORDS = 100000 # Vocabulary size\n",
        "    EMBEDDING_DIM = 1000 # Dimensions of Glove word vectors \n",
        "    VALIDATION_SPLIT = 0.10\n",
        "\n",
        "    filter_sizes = [2,3,5]\n",
        "    num_filters = 512\n",
        "    drop = 0.5\n",
        "\n",
        "    inputs = Input(shape=(4096,), dtype='int32')\n",
        "    # embedding = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "    #                       input_length=MAX_SEQUENCE_LENGTH, trainable=False)(inputs)\n",
        "    # reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(inputs)\n",
        "\n",
        "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 4096), padding='valid', kernel_initializer='normal', activation='relu')(inputs)\n",
        "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 4096), padding='valid', kernel_initializer='normal', activation='relu')(inputs)\n",
        "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], 4096), padding='valid', kernel_initializer='normal', activation='relu')(inputs)\n",
        "\n",
        "    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "    maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "    maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "    flatten = Flatten()(concatenated_tensor)\n",
        "    dropout = Dropout(drop)(flatten)\n",
        "    preds = Dense(num_output, activation='softmax')(dropout)\n",
        "    adamOpti = Adam()\n",
        "    preds.compile(optimizer=adamOpti, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    preds.summary()\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0qw38xao2F8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c31a5575-6e1c-4142-a817-37bbee57c4c7"
      },
      "source": [
        "%%time\n",
        "model = generate_Infersent_model()\n",
        "# model = torch.load('infraset_model.torch')\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 100000\n",
            "CPU times: user 6.43 s, sys: 751 ms, total: 7.19 s\n",
            "Wall time: 7.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ieDDJwo3Gc"
      },
      "source": [
        "\n",
        "X_train_vec = get_doc2vec(X_train, model)\n",
        "X_val_vec = get_doc2vec(X_val, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY77zk1qEWSd"
      },
      "source": [
        "np.save('X_train_reminders_vec.npy', X_train_vec) # save\n",
        "np.save('X_val_reminders_vec.npy', X_val_vec) # save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6D3Wm89ueHp"
      },
      "source": [
        "y_train_one_hot = np.array(y_train_one_hot)\n",
        "y_val_one_hot = np.array(y_val_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQB0M0TBEO6H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtqXv8YVpQKC"
      },
      "source": [
        "\n",
        "ffcc = generate_basic_fully_connected()\n",
        "ffcc.fit(X_train_vec, y_train_one_hot, \n",
        "         epochs=100, batch_size=32, shuffle=True,\n",
        "         validation_data=(X_val_vec, y_val_one_hot)\n",
        "         , callbacks = [EarlyStopping(monitor='val_loss', patience=12)])\n",
        "ffcc.save('fcc_keras_reminders_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlB3eOc6pVHr"
      },
      "source": [
        "def get_label_decoded(index):\n",
        "    return [\"reminder\", \"oos\"][index]\n",
        "\n",
        "def get_labels_decoded(labels):\n",
        "    results = np.argmax(labels, axis=1)\n",
        "    decoded = []\n",
        "    for label in results:\n",
        "        label = get_label_decoded(label)\n",
        "        decoded.append(label)\n",
        "    return decoded\n",
        "\n",
        "def calculate_accuracy(model, X_vec, y_labels, nueral_net=True, with_oos=True, ignore_correct=True, cutoff=0.7):\n",
        "    predictions = model.predict(X_vec)\n",
        "    predicted_labels = get_labels_decoded(predictions) # each integer is [0, 0, 0,1]\n",
        "    posterior_prob = np.max(predictions, axis=1)\n",
        "        # posterior_prob =  \n",
        "    correct = 0\n",
        "#     classification = {key:defaultdict(int) for key in y_labels}\n",
        "    classification = []\n",
        "    num_oos = 0\n",
        "    for pred,y_true, prob in zip(predicted_labels, y_labels, posterior_prob):\n",
        "        # if prob < cutoff:\n",
        "        #     pred = 'oos'\n",
        "        if y_true == 'oos' and not with_oos:\n",
        "          num_oos += 1\n",
        "          continue\n",
        "        if pred == y_true:\n",
        "            correct += 1\n",
        "            if ignore_correct:\n",
        "              continue\n",
        "        classification.append({\"y_true\": y_true, \"pred\": pred, \"prob\": prob})\n",
        "\n",
        "    classification = pd.DataFrame(classification)\n",
        "    classification = classification.groupby(['y_true','pred']).agg({'prob':[('prob', 'mean')], 'pred': [('count','count')]}).reset_index()\n",
        "    classification.columns = [\"y_true\", \"pred\", \"prob\", \"count\"]\n",
        "    print(classification)\n",
        "\n",
        "    accuracy = correct/(len(predictions) - num_oos)\n",
        "    \n",
        "    return accuracy, classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udp9ETGS1Wcl"
      },
      "source": [
        "y_train_labels = get_labels_decoded(y_train_one_hot)\n",
        "y_val_labels = get_labels_decoded(y_val_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7M--vIerNkt"
      },
      "source": [
        "def analyze_missclassifications(classifications):\n",
        "  dic = {}\n",
        "  for index, row in classifications.iterrows():\n",
        "      y_true = row['y_true']\n",
        "      pred = row['pred']\n",
        "      posterior_prob = row['prob']\n",
        "      count = row['count']\n",
        "      if y_true not in dic:\n",
        "        dic[y_true] = {\"count\": 0}\n",
        "      dic[y_true][\"count\"] += count\n",
        "  \n",
        "      if y_true == pred:\n",
        "        dic[y_true][\"true count\"] = count\n",
        "        dic[y_true][\"confidence\"] = posterior_prob\n",
        "  l = []\n",
        "  for key, val in dic.items():\n",
        "      if \"true count\" not in val:\n",
        "        dic[key][\"true count\"] = 0\n",
        "        dic[key][\"confidence\"] = 0.0\n",
        "      dic[key][\"TP\"] = dic[key][\"true count\"]/dic[key][\"count\"]\n",
        "      l.append({\"y_true\": key, \"confidence\": dic[key][\"confidence\"], \"count\": dic[key][\"count\"], \"TP\": dic[key][\"TP\"]})\n",
        "  res = pd.DataFrame(l).sort_values(['confidence', 'TP', 'count'],ascending=False)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWCviALjrsSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6b36cb58-02df-4504-e949-422a29237b04"
      },
      "source": [
        "%%time\n",
        "train_accuracy, train_classifications = calculate_accuracy(ffcc, X_train_vec, y_train_labels, ignore_correct=False, with_oos=True)\n",
        "print(\"Train Accuracy With OOS\", train_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     y_true      pred      prob  count\n",
            "0       oos       oos  0.999693   2275\n",
            "1  reminder       oos  0.879694      1\n",
            "2  reminder  reminder  0.999871   2021\n",
            "Train Accuracy With OOS 0.9997672794973237\n",
            "CPU times: user 227 ms, sys: 27.3 ms, total: 254 ms\n",
            "Wall time: 208 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_fCqKfZ2idT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4b8d2dd7-5ace-4e39-cf6e-d723ebd1e6e3"
      },
      "source": [
        "%%time\n",
        "validation_accuracy, validation_classifications = calculate_accuracy(ffcc, X_val_vec, y_val_labels, ignore_correct=False, with_oos=True)\n",
        "print(\"Validation Accuracy With OOS\", validation_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     y_true      pred      prob  count\n",
            "0       oos       oos  0.999464    241\n",
            "1       oos  reminder  0.998382      1\n",
            "2  reminder       oos  0.974393      4\n",
            "3  reminder  reminder  0.999044    226\n",
            "Validation Accuracy With OOS 0.989406779661017\n",
            "CPU times: user 63.1 ms, sys: 1.04 ms, total: 64.1 ms\n",
            "Wall time: 57.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd8pZoCD228y"
      },
      "source": [
        "def quick_predict_label(infraset_model, classification_model, text, cutoff=0.0):\n",
        "    X = get_doc2vec([text], infraset_model)\n",
        "    predictions = classification_model.predict(X)\n",
        "    predicted_labels = get_labels_decoded(predictions) # each integer is [0, 0, 0,1]\n",
        "    prob = np.max(predictions, axis=1) # each integer is [0, 0, 0,1]\n",
        "    if prob < cutoff:\n",
        "      predicted_labels[0] = 'oos' # since only one item\n",
        "    return predicted_labels, prob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgTG3TtJ3G7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c5e779c-f26a-40d7-e632-9e641f8654a5"
      },
      "source": [
        "quick_predict_label(model, ffcc, \"At what time is the dataset done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['oos'], array([0.99995685], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga91NXD43Jpw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36cd25e7-8448-4479-8ae2-718292e869e4"
      },
      "source": [
        "quick_predict_label(model, ffcc, \"Reminder me to go to the Park\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['reminder'], array([1.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nbdBj247rev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "06e32b40-5b1e-4426-a60a-9188c0491015"
      },
      "source": [
        "!pip install padatious"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting padatious\n",
            "  Using cached https://files.pythonhosted.org/packages/d0/e7/70a6eb34b7e67fef5b2645df2ee1f807db2b5a345e4e6adfb2660a56425b/padatious-0.4.8.tar.gz\n",
            "Collecting fann2\n",
            "  Using cached https://files.pythonhosted.org/packages/80/a1/fed455d25c34a62d4625254880f052502a49461a5dd1b80854387ae2b25f/fann2-1.1.2.tar.gz\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/0e/5c6388b52e514620bdaf6696fd815e44f118a3fb312e8f1b22788eeadfa0/xxhash-1.4.4-cp36-cp36m-manylinux2010_x86_64.whl (217kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 9.6MB/s \n",
            "\u001b[?25hCollecting padaos\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/32/5a9d9110a1ad4731bcba99c9b1291e29d740b45717bd5b5ba7e72627008f/padaos-0.1.10.tar.gz\n",
            "Building wheels for collected packages: padatious, fann2, padaos\n",
            "  Building wheel for padatious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for padatious: filename=padatious-0.4.8-py2.py3-none-any.whl size=24603 sha256=5cdee3d71f0c0c5777f1dba4d35949fcbd3f42e7749b8c21bbfd91bf6f063046\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/aa/ed/5dbb2bfbbabf2d13ffaa4b4e3bda2f8f97e621df189461f978\n",
            "  Building wheel for fann2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fann2: filename=fann2-1.1.2-cp36-cp36m-linux_x86_64.whl size=299043 sha256=18a5353e946d81d4620c030b21bab6a0ad7624cdca5be033040164e13d9ec865\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/90/e3/b97a2310a956f17c56048801aabedafcffbee48ff62d0374ed\n",
            "  Building wheel for padaos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for padaos: filename=padaos-0.1.10-cp36-none-any.whl size=3150 sha256=3a6c5f2b8083b1251be69f246a1e12b637250ca05e8e4b5f1c44cb0af94d2c96\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/68/87/2cf37b178bd4734a8db2e4ac15452120e2d1dcc4b5bf01d751\n",
            "Successfully built padatious fann2 padaos\n",
            "Installing collected packages: fann2, xxhash, padaos, padatious\n",
            "Successfully installed fann2-1.1.2 padaos-0.1.10 padatious-0.4.8 xxhash-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93rcxjWfXB4F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d83b4433-954a-4e02-a241-d17a81b1cd27"
      },
      "source": [
        "from padatious import IntentContainer\n",
        "\n",
        "container = IntentContainer('intent_cache')\n",
        "container.add_intent('reminder', X_train_reminder)\n",
        "# container.add_intent('oos', [item.replace(\".\", \" \").replace(\":\", \" \").replace(\")\", \" \").replace(\"(\", \" \").replace(\"!\", \" \").replace(\";\", \" \").replace(\",\", \" \") for item in train_oos])\n",
        "# container.add_intent('search', ['Search for {query} (using|on) {engine}.'])\n",
        "container.train()\n",
        "\n",
        "print(container.calc_intent('Remind me to go to the park tommorow afternoon with the kids'))\n",
        "# print(container.calc_intent('At what time with the data be finished'))\n",
        "\n",
        "# container.remove_intent('goodbye')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'reminder', 'sent': 'remind me to go to the park tommorow afternoon with the kids', 'matches': {}, 'conf': 0.6059698669580462}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59KtApRkXTKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fd1b44d-0c39-4c30-8a8c-54cca97aa211"
      },
      "source": [
        "len(X_train_reminder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4320"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9wkMaXYu2r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kub3A5tYYyMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3345f35e-3f29-4a45-cb1b-16bd09e7cdb9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q_sOFajYy_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "cad3ee40-0fb9-440f-8983-13cefca2a7e4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libfann-dev is already the newest version (2.2.0+ds-3).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAZhXioPY1Ob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cb28ef9-b25f-4316-8a05-f1c9cab37e2a"
      },
      "source": [
        "!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[35m\u001b[1mScanning dependencies of target floatfann\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object src/CMakeFiles/floatfann.dir/floatfann.c.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking C shared library libfloatfann.so\u001b[0m\n",
            "[  8%] Built target floatfann\n",
            "\u001b[35m\u001b[1mScanning dependencies of target floatfann_static\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object src/CMakeFiles/floatfann_static.dir/floatfann.c.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking C static library libfloatfann.a\u001b[0m\n",
            "[ 16%] Built target floatfann_static\n",
            "\u001b[35m\u001b[1mScanning dependencies of target doublefann\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object src/CMakeFiles/doublefann.dir/doublefann.c.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking C shared library libdoublefann.so\u001b[0m\n",
            "[ 24%] Built target doublefann\n",
            "\u001b[35m\u001b[1mScanning dependencies of target doublefann_static\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding C object src/CMakeFiles/doublefann_static.dir/doublefann.c.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking C static library libdoublefann.a\u001b[0m\n",
            "[ 32%] Built target doublefann_static\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fixedfann\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding C object src/CMakeFiles/fixedfann.dir/fixedfann.c.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking C shared library libfixedfann.so\u001b[0m\n",
            "[ 40%] Built target fixedfann\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fixedfann_static\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding C object src/CMakeFiles/fixedfann_static.dir/fixedfann.c.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking C static library libfixedfann.a\u001b[0m\n",
            "[ 48%] Built target fixedfann_static\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fann\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding C object src/CMakeFiles/fann.dir/floatfann.c.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking C shared library libfann.so\u001b[0m\n",
            "[ 56%] Built target fann\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fann_static\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding C object src/CMakeFiles/fann_static.dir/floatfann.c.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking C static library libfann.a\u001b[0m\n",
            "[ 64%] Built target fann_static\n",
            "\u001b[35m\u001b[1mScanning dependencies of target gtest\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lib/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX shared library libgtest.so\u001b[0m\n",
            "[ 72%] Built target gtest\n",
            "\u001b[35m\u001b[1mScanning dependencies of target gtest_main\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lib/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX shared library libgtest_main.so\u001b[0m\n",
            "[ 80%] Built target gtest_main\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fann_tests\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/fann_tests.dir/main.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object tests/CMakeFiles/fann_tests.dir/fann_test.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object tests/CMakeFiles/fann_tests.dir/fann_test_data.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tests/CMakeFiles/fann_tests.dir/fann_test_train.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable fann_tests\u001b[0m\n",
            "[100%] Built target fann_tests\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /usr/local/lib/pkgconfig/fann.pc\n",
            "-- Installing: /usr/local/lib/cmake/fann/fann-config.cmake\n",
            "-- Installing: /usr/local/lib/cmake/fann/fann-use.cmake\n",
            "-- Installing: /usr/local/lib/libfloatfann.so.2.2.0\n",
            "-- Installing: /usr/local/lib/libfloatfann.so.2\n",
            "-- Installing: /usr/local/lib/libfloatfann.so\n",
            "-- Installing: /usr/local/lib/libfloatfann.a\n",
            "-- Installing: /usr/local/lib/libdoublefann.so.2.2.0\n",
            "-- Installing: /usr/local/lib/libdoublefann.so.2\n",
            "-- Installing: /usr/local/lib/libdoublefann.so\n",
            "-- Installing: /usr/local/lib/libdoublefann.a\n",
            "-- Installing: /usr/local/lib/libfixedfann.so.2.2.0\n",
            "-- Installing: /usr/local/lib/libfixedfann.so.2\n",
            "-- Installing: /usr/local/lib/libfixedfann.so\n",
            "-- Installing: /usr/local/lib/libfixedfann.a\n",
            "-- Installing: /usr/local/lib/libfann.so.2.2.0\n",
            "-- Installing: /usr/local/lib/libfann.so.2\n",
            "-- Installing: /usr/local/lib/libfann.so\n",
            "-- Installing: /usr/local/lib/libfann.a\n",
            "-- Installing: /usr/local/include/fann.h\n",
            "-- Installing: /usr/local/include/doublefann.h\n",
            "-- Installing: /usr/local/include/fann_internal.h\n",
            "-- Installing: /usr/local/include/floatfann.h\n",
            "-- Installing: /usr/local/include/fann_data.h\n",
            "-- Installing: /usr/local/include/fixedfann.h\n",
            "-- Installing: /usr/local/include/fann_activation.h\n",
            "-- Installing: /usr/local/include/fann_cascade.h\n",
            "-- Installing: /usr/local/include/fann_error.h\n",
            "-- Installing: /usr/local/include/fann_train.h\n",
            "-- Installing: /usr/local/include/fann_io.h\n",
            "-- Installing: /usr/local/include/fann_cpp.h\n",
            "-- Installing: /usr/local/include/fann_data_cpp.h\n",
            "-- Installing: /usr/local/include/fann_training_data_cpp.h\n",
            "-- Installing: /usr/local/include/parallel_fann.h\n",
            "-- Installing: /usr/local/include/parallel_fann.hpp\n",
            "make: *** No rule to make target 'fane/'.  Stop.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqy0edxJY4G6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}